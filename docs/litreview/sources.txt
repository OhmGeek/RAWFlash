-----------------------------------------------------------------------------
Article
It's all about the format – unleashing the power of RAW aerial photography

Verhoeven, G. J. J. • 2010

Published in Journal International Journal of Remote Sensing. Volume 31. Issue 8. Page 2009 - 2042.


Abstract:
Current one-shot, handheld Digital Still Cameras (DSCs) generally offer different file formats to save the captured frames: Joint Photographic Experts Group (JPEG), RAW and/or Tag(ged) Image File Format (TIFF). Although the JPEG file format is the most commonly used file format worldwide, it is incapable of storing all original data, something that also occurs, to a certain extent, for large TIFF files. Therefore, most professional photographers prefer shooting RAW files, often described as the digital photography's equivalent of a film negative. As a RAW file contains the absolute maximum amount of information and original data generated by the sensor, it is the only scientifically justifiable file format. In addition, its tremendous flexibility in both processing and post-processing also makes it beneficial from a workflow and image quality point of view. On the other hand, large file sizes, the required software and proprietary file formats remain hurdles that are often too difficult to overcome for many photographers. Aerial photographers who shoot with handheld DSCs should be familiar with both RAW and other file formats, as their implications cannot be neglected. By outlining the complete process from photon capture to the generation of pixel values, additionally illustrated by real-world examples, the advantages and particularities of RAW aerial photography should become clear.

Useful Info: TO SUMMARISE FURTHER TO AVOID PLAGIARISM
- RAW files contain the analogue sensor data that has been amplified and converted to digital data, rather like a Digital Negative, allowing an infinite number of prints, without any loss in quality.
- In a camera, CCDs, CMOS or JFET sensors are used, all of which are different methods of obtaining a measure of the light hitting the sensor. They follow the same principle of being a silicon chip, in a 2D array of photo diodes.
- From exposure, the photo diodes college photons, gathered by the lens. EM radiation is sampled spatially (location within the image), tonally (measuring the number of photons captured, using some metric), and with time (known as the exposure time).
- Each photo diode can only measure the amount of light, not the wavelength. Hence, an image would only be able to create greyscale photographs. This problem is solved by using a Color Filter Array (CFA). A CFA is a mosaic of coloured filters, positioned on top of the sensor, allowing only certain wavelengths to be collected per sensor. Common arrangement is the bayer pattern, with a repeating group of 4 photo diodes, two which have green filters, with another as blue and another as red, to mimic the higher sensitivity to green light of the human.
Other patterns exist too, using CMY or RGB, along with four colour systems.
Some cameras, like Polaroid's x530 and Sigma's DSLR SD9, SD10 and SD14 use the X3** by Foveon Inc, which is a three layered CMOS image sensor with a stack of three photo diodes at each photosite, capturing the R,G and B wave-bands at the same location.
Using the values from each colour, applying the photodiode value to the colour filter, we can measure the buildup of charge for each photo diode. These small analogue voltages are amplified by a read-out amplifier with a certain gain, corresponding to the ISO value set.
ISO value expresses the sensitivity of photographic film and sensors on a numeric scale.

RAW also allows us to apply a specific colour space, such as sRGB or Adobe RGB. One can also tackle noise using certain algorithms, and perform sharpening. TIFF and JPEG are formats designed as 'raster file formats', designed to store still images as photographs, while RAW is a 'latent image data' store. RAW also can store metadata such as intensity information.

JPEG uses compression, either lossy or lossless. Lossless JPEG is not that popular, as it can only compress full colour data by 2:1 (as specified in the source). Lossy JPEG has a typical compression of 10:1, making very small files. However, the file itself can deteriorate in quality. It also applies a tonal curve, which potentially means the entire Dynamic Range of the camera isn't being stored. In RAW, the tonal curve of the camera is stored, getting the best possible value of the curve. Furthermore, highlight information is also stored with RAW but lost in JPEG compression.

TIFF stores all the data in the correct order, with all of the captured colours, providing no lossless compression is used. Furthermore, uncompressed TIFFs are no affected by the data loss found in JPEGs. One major drawback of this file format is the file size, with a maximum limit of 4GB per image. Furthermore, as the file format contains a flexible tag structure, this can sometimes lead to problems interpreting the file, as new tags added might cause errors in programs. While TIFF files are better than RAW files, the interpretation of the scene is already performed, unlike with RAW.

RAW as a format can be used to create lower quality JPEG files, or TIFF files as output. Some formats like Nikon's NEF format include an Exif JPEG file.

However, as RAW is not a standard, only a file format interpreted and changed depending on the camera manufacturer, it can become hard to standadise. Two recommendations for current camera converters are dcraw and UFRaw, alogn with UFRaw which is open source.

RAW is ultimately proprietary, and while there are standards such as Adobe's DNG, these aren't widely implemented. The OpenRAW group exists to compel camera manufacturers to publically document their RAW image formats.
** Registered trademark, so use the (R) symbol.

Not so useful info:
Anything related directly with Aerial Photographers, as this is a particular area
of photography we aren't directly interested in, as we are building photo editing software
in general.

-------------------------------------------------------------------------------------
Book Chapter 6

Olivier Losson, Eric Dinet. From the Sensor to Color Images. Christine Fernandez-Maloigne,
Fr´ed´erique Robert-Inacio, Ludovic Macaire. Digital Color - Acquisition, Perception, Coding
and Rendering, Wiley, pp.149-185, 2012, Digital Image and Signal Processing series.

Introduction:
In Chapter 5, we saw how color sensors convert a light radiation
into an electrical signal to build a digital image. It describes
the two currently most widespread sensor technologies (CCD
and CMOS) as well as the emerging technologies offered by
some manufacturers. These color sensors are nowadays ubiquitous
in the objects of everyday life, but the acquisition devices
equipped with three sensors are overwhelmingly confined to the
professional sectors and to very specific applications. Owing to the
complexity of their manufacture, these devices are costly and largely
unaffordable by the public. A whole range of so-called “hi-tech”
products such as digital cameras, mobile phones, and computers are
thus equipped with a single sensor to form a color image, and so are
many professional devices such as quality control cameras and video
surveillance cameras. One reason for this is the dramatic advances in
the operation aiming to obtain a color image from the sensor data.
The operation in question, known as demosaicing, is the subject of this
chapter.

Devices equipped with a single sensor form a color image by
estimating it from the so-called raw image or color filter array (CFA)
image, in which each pixel is only characterized by a single bit of
color information. Specifically, the filter mosaic of the CFA samples
a single color component (red, green, or blue) at each photoreceptor,
and demosaicing aims to estimate the two missing components at the
corresponding pixel. This is a far from trivial operation, and the colors
estimated thus are less consistent with the color stimuli of the observed
scene than those provided by a three-sensor camera. Improving the
fidelity of the color image is still a current issue, on which scientists and
engineers are working. To obtain an image rendering the colors of the
scene as accurately as possible, other processings are typically integrated
into the acquisition system, foremost among which are calibration and
color correction.

In the following pages, we look at the formation of color images from
the data delivered by the sensor, and at the fidelity of these images to the
observed scene. The first three sections, largely inspired by the work of
Yanqin Yang [YAN 09], deal with the problem of demosaicing, while the
last part tackles the problem of color camera calibration. After setting
a few notations, in the first section, we present two principles used by
the majority of demosaicing methods. The second section presents the
key ideas of the main demosaicing algorithms. The issue of fidelity of
the estimated image is discussed in the third section, which presents
both the main measurement criteria and some results allowing us to
select a demosaicing method. Finally, we examine the need for and the
implementation of the processing known as white balance, usually done
before the demosaicing, to obtain a color image that is faithful to the
scene regardless of illumination conditions.


Useful Info:

- There are two different types of demosaicing algorithm; one class uses the spatial domain, while another uses the frequency domain (need better words here). In particular, for a single sensor camera, which typical consumer grade DSLRs are.


- Over time, as demosaicing algorithms have improved, the quality of the image produced has increased, allowing a larger similarity between the initial scene, and the resulting image from demosaicing.

- The source found that algorithms utilising the frequency domain of the RAW image, produces a better image in terms of signal to noise ratio. This is also produced for a relatively low cost computationally, meaning this might potentially apply well to our software in terms of RAW image processing, taking a massive improvement over spacial analysis.

- The frequency analysis method is proposed by Alleysson et al.


Not so useful/flaws

- The source itself doesn't apply itself to the ultimate quality of the image display/reproduction, which is what we will ultimately doing to it. This is due to a greater control over human visual perception, which is unlikely.

----------------------------------------------------------
'Dynamic Web Service Based Image Processing System'

Basic info:
System consists of a schedule describing the possible operations, and the URL of the web service providing the image processing service.



Useful
- Uses web services to process images, exactly what our system will be doing.

Not so useful:

- Few citations, so either that means it's obscure or not as useful as other sources
- Very S3 oriented, towards Amazon Web Services. We might not directly be doing this.
